import numpy as np
import os
import matplotlib.pyplot as plt
import pandas as pd
import sigmoid_function as sf

'''
First, verify that the gradient of the log-loss with respect to the weight vector is:
âˆ‚ğ‘ğ¿ğ¿(ğ—,ğ²,ğ°,ğ‘)âˆ‚ğ°=âˆ‘ğ‘–=1ğ‘›âˆ’ğ‘¦ğ‘–ğœ(âˆ’ğ‘¦ğ‘–(ğ°âŠ¤ğ±ğ‘–+ğ‘))ğ±ğ‘–
âˆ‚ğ‘ğ¿ğ¿(ğ—,ğ²,ğ°,ğ‘)âˆ‚ğ‘=âˆ‘ğ‘–=1ğ‘›âˆ’ğ‘¦ğ‘–ğœ(âˆ’ğ‘¦ğ‘–(ğ°âŠ¤ğ±ğ‘–+ğ‘))
Implement the function gradient, which returns the first derivative with respect to w, b given X, y, w, b. You have access to sigmoid_grader function that returns  ğœ(ğ‘§) .

Recall how we got the expressions for the gradient:
âˆ‚ğ‘ğ¿ğ¿âˆ‚ğ°=âˆ’âˆ‚[âˆ‘ğ‘›ğ‘–=1logğœ(ğ‘¦ğ‘–(ğ°âŠ¤ğ±ğ‘–+ğ‘))]âˆ‚ğ°=âˆ’âˆ‘ğ‘–=1ğ‘›âˆ‚[logğœ(ğ‘¦ğ‘–(ğ°âŠ¤ğ±ğ‘–+ğ‘))]âˆ‚ğ°=âˆ’âˆ‘ğ‘–=1ğ‘›1ğœ(ğ‘¦ğ‘–(ğ°âŠ¤ğ±ğ‘–+ğ‘))âˆ‚[ğœ(ğ‘¦ğ‘–(ğ°âŠ¤ğ±ğ‘–+ğ‘))]âˆ‚ğ°=âˆ’âˆ‘ğ‘–=1ğ‘›ğœâ€²(ğ‘¦ğ‘–(ğ°âŠ¤ğ±ğ‘–+ğ‘))ğœ(ğ‘¦ğ‘–(ğ°âŠ¤ğ±ğ‘–+ğ‘))â‹…ğ‘¦ğ‘–ğ±ğ‘–.
Since we know that  ğœâ€²(ğ‘§)=ğœ(ğ‘§)(1âˆ’ğœ(ğ‘§))  and that  1âˆ’ğœ(ğ‘§)=ğœ(âˆ’ğ‘§) , our expression finally becomes:
âˆ‚ğ‘ğ¿ğ¿âˆ‚ğ°=âˆ’âˆ‘ğ‘–=1ğ‘›ğœ(âˆ’ğ‘¦ğ‘–(ğ°âŠ¤ğ±ğ‘–+ğ‘))â‹…ğ‘¦ğ‘–ğ±ğ‘–.
You can similarly derive the gradient w.r.t.  ğ‘  as an exercise.

def gradient(X, y, w, b):

'''


def gradient(X, y, w, b):
    '''
    Calculates the gradients of NLL w.r.t. w and b and returns (w_grad, bgrad).

    Input:
        X: data matrix of shape nxd
        y: n-dimensional vector of labels (+1 or -1)
        w: d-dimensional weight vector
        b: scalar bias term

    Output:
        wgrad: d-dimensional vector (gradient vector of w)
        bgrad: a scalar (gradient of b)
    '''
    n, d = X.shape
    wgrad = np.zeros(d)
    bgrad = 0.0

    # YOUR CODE HERE
    temp = np.dot(X, w) + b
    sig = sf.sigmoid(-1 * y * temp)
    sig = sig.reshape(n, 1)
    #     print(sig)
    y = y.reshape(n, 1)
    #     print(y)
    #     wgrad = np.sum(np.dot(np.dot(X.transpose(),sig),y.transpose()*(-1)), axis = 1)
    wgrad = np.sum(np.dot((-y * sig).transpose(), X), axis=0)
    bgrad = np.sum(-y * sig)
    return wgrad, bgrad

